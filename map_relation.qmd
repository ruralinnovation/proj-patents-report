---
title: "Relations between tables"
date: last-modified
format: 
  html:
    code-fold: true
engine: knitr
reference-location: margin
---


```{r}
#| label: load lib
library(dm, warn.conflicts = FALSE)
library(htmltools)
library(DiagrammeRsvg)
```

# EDA: Exploratory Data Analysis

## First step: download nearly all data sets.

Here we have room for improvement if we want to automate it.

 - this could be a pot. improvement: https://books.ropensci.org/targets/cloud-storage.html 

## Second step: get a sample of all of them. 

I went with the first 5 rows.   

```bash
# 1. create some repos
mkdir -p data/data_raw/unziped
mkdir -p data/data_sample/
# 2. I could find a way to unzip just the first rows
unzip data/data_raw/\*.zip -d data/data_raw/unziped
# 3. Get what we need
for file in data/data_raw/unziped/*.tsv ; do
        head -n5 "$file" >  "$file.head"
        echo "$file.head" 
done
# 4. Store it were we need it
mv data/data_raw/unziped/*.head data/data_sample
# 5. delete not needed and rename
rm -rf data/data_raw/unziped
```

## Third: read everything in R. 

```{r}
#| label: read all tsv
read_sample <- function(path_dir){
    list_tsv <- paste0(path_dir,
                    list.files(path_dir, 
                                pattern = ".tsv"))

    list_samples <- sapply(list_tsv, read.csv, sep ="\t") 
    names(list_samples) <- basename(names(list_samples))
    return(list_samples)
}

list_samples <- read_sample("data/data_sample/") 

# list2env(list_samples, .GlobalEnv)
names(list_samples)
``` 

## Forth: use DM to do a nice schema

```{r}
# simplify my list I went back and forth
i_data <- c("g_assignee_disambiguated.tsv.head",
            "g_inventor_disambiguated.tsv.head", 
            "g_location_disambiguated.tsv.head", 
            "g_patent.tsv.head")

# yes dm can also just take a list of df
pot_schema_no_keys <- as_dm(list_samples[i_data])
```

Sadly I need to first test what are the primary and foreign key (PK and FK) and if I can trust the data about that: meaning reading a bunch of big csv and not only sampling to build the schema.

I am first trying to get:

- county

- year

- nb of inventor

- nb of assignee (organisation)

- nb of patents â†’ using assignee for localisation


dm offer high level function taht return tibble with result for the all table:

```{r}
#| eval: false
path_to_tsv <- "data/data_raw/unziped/"
#  
g_application <- data.table::fread(paste0(path_to_tsv, "g_application.tsv"), sep = "\t")
enum_pk_candidates(g_application)

g_assignee_dis <- data.table::fread(paste0(path_to_tsv,
                                           "g_assignee_disambiguated.tsv"), 
                                    sep ="\t")
assignee_sum <- enum_pk_candidates(g_assignee_dis)
# no pk but should have patent_id / assignee_id and location id has fk
g_inventor_dis <- data.table::fread(paste0(path_to_tsv, "g_inventor_disambiguated.tsv"), sep = "\t")
enum_pk_candidates(g_inventor_dis)
# no pk ..
g_location_dis <- data.table::fread(paste0(path_to_tsv, "g_location_disambiguated.tsv"), sep = "\t") 
enum_pk_candidates(g_location_dis)

g_patent <- data.table::fread(paste0(path_to_tsv, "g_patent.tsv"), sep = "\t")
enum_pk_candidates(g_patent)
# location ID

```


But also more targeted functions:

```{r}
#| eval: false
(check_key(g_location_dis, location_id)) # a bit of strange behavior it return null on success?
# test if we have value in g_assignee_dis$location_id not in g_location_dis 
# tests if x is a subset of y
check_subset(g_assignee_dis, location_id, g_location_dis, location_id)
nrow(g_assignee_dis[g_assignee_dis$location_id == "",])
#  90823
nrow(g_assignee_dis)
# 8206092 
# small number does not have location 
check_subset(g_assignee_dis[!g_assignee_dis$location_id == "",], location_id, g_location_dis, location_id)
# return correct hence if we want use location as a fk for assignee we need to do that 
(check_key(g_location_dis, location_id)) 
## patent id 
check_subset(g_assignee_dis[!g_assignee_dis$location_id == "",], patent_id, g_patent, patent_id)
check_subset(g_inventor_dis, patent_id, g_patent, patent_id)
```

Slowly adding them:

```{r}
pot_schema_pk_keys <- 
    pot_schema_no_keys |>
        dm_add_pk(g_location_disambiguated.tsv.head, columns = location_id) |> 
        dm_add_pk(g_patent.tsv.head, columns = patent_id)
```


Adding Foreign keys:


```{r}
#| label: add foreign key
pot_schema_all_keys <- 
    pot_schema_pk_keys |>
     dm_add_fk(g_inventor_disambiguated.tsv.head, 
               location_id,
               g_location_disambiguated.tsv.head) |>
    dm_add_fk(g_assignee_disambiguated.tsv.head,
              location_id,
              g_location_disambiguated.tsv.head) |> 
    dm_add_fk(g_assignee_disambiguated.tsv.head,
              patent_id,
              g_patent.tsv.head) |>
    dm_add_fk(g_inventor_disambiguated.tsv.head,
              patent_id,
              g_patent.tsv.head)
```

## Five: Try to render that schema:

```{r}
#| label: display schema

schema <- dm_draw(pot_schema_all_keys, view_type = 'all')

# kind of hacky 
schema_html <- DiagrammeRsvg::export_svg(schema)
htmltools::HTML(schema_html )
```

# Producing a summary table

 Sixth: given that logic build a summary table by county/year

 We will probably do it in at least to n steps:

 - Getting counties, yes right now we do not know if we are missing some counties and we are still unsure what vintage of census patentsview is using.

 - linking patent to assignee then link to location

 - linking patent to inventore then link to location 

