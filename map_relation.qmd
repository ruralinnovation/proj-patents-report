---
title: "Relations between tables"
date: last-modified
format: 
  html:
    code-fold: true
engine: knitr
reference-location: margin
---


```{r}
#| label: load lib
library(dm, warn.conflicts = FALSE)
library(htmltools)
library(DiagrammeRsvg)

source("R/utils.R")
```

# EDA: Exploratory Data Analysis

## First step: download nearly all data sets.

Here we have room for improvement if we want to automate it.

 - this could be a pot. improvement: https://books.ropensci.org/targets/cloud-storage.html 

## Second step: get a sample of all of them. 

I went with the first 5 rows.   

```bash
# 1. create some repos
mkdir -p data/data_raw/unziped
mkdir -p data/data_sample/
# 2. I could find a way to unzip just the first rows
unzip data/data_raw/\*.zip -d data/data_raw/unziped
# 3. Get what we need
for file in data/data_raw/unziped/*.tsv ; do
        head -n5 "$file" >  "$file.head"
        echo "$file.head" 
done
# 4. Store it were we need it
mv data/data_raw/unziped/*.head data/data_sample
# 5. delete not needed and rename
rm -rf data/data_raw/unziped
```

## Third: read everything in R. 

```{r}
#| label: read all tsv
list_samples <- read_sample("data/data_sample/") 
names(list_samples)
``` 

## Forth: use DM to do a nice schema

```{r}
# simplify my list I went back and forth
i_data <- c("g_assignee_disambiguated.tsv.head",
            "g_inventor_disambiguated.tsv.head", 
            "g_location_disambiguated.tsv.head", 
            "g_patent.tsv.head")

# yes dm can also just take a list of df
pot_schema_no_keys <- as_dm(list_samples[i_data])
```

Sadly I need to first test what are the primary and foreign key (PK and FK) and if I can trust the data about that: meaning reading a bunch of big csv and not only sampling to build the schema.

I am first trying to get:

- county

- year

- nb of inventor

- nb of assignee (organisation)

- nb of patents â†’ using assignee for localisation


dm offer high level functions that return tibble with result for all tables:

- `enum_pk_candidates(a_table)`
- and more targets function:

```{r}
#| eval: false
(check_key(g_location_dis, location_id)) # a bit of strange behavior it return null on success?
# test if we have value in g_assignee_dis$location_id not in g_location_dis 
# tests if x is a subset of y
check_subset(g_assignee_dis, location_id, g_location_dis, location_id)
nrow(g_assignee_dis[g_assignee_dis$location_id == "",])
#  90823
nrow(g_assignee_dis)
# 8206092 
# small number does not have location 
check_subset(g_assignee_dis[!g_assignee_dis$location_id == "",], location_id, g_location_dis, location_id)
# return correct hence if we want use location as a fk for assignee we need to do that 
(check_key(g_location_dis, location_id)) 
## patent id 
check_subset(g_assignee_dis[!g_assignee_dis$location_id == "",], patent_id, g_patent, patent_id)
check_subset(g_inventor_dis, patent_id, g_patent, patent_id)
```

Slowly adding them:

```{r}
pot_schema_pk_keys <- 
    pot_schema_no_keys |>
        dm_add_pk(g_location_disambiguated.tsv.head, columns = location_id) |> 
        dm_add_pk(g_patent.tsv.head, columns = patent_id)
```


Adding Foreign keys:


```{r}
#| label: add foreign key
pot_schema_all_keys <- 
    pot_schema_pk_keys |>
     dm_add_fk(g_inventor_disambiguated.tsv.head, 
               location_id,
               g_location_disambiguated.tsv.head) |>
    dm_add_fk(g_assignee_disambiguated.tsv.head,
              location_id,
              g_location_disambiguated.tsv.head) |> 
    dm_add_fk(g_assignee_disambiguated.tsv.head,
              patent_id,
              g_patent.tsv.head) |>
    dm_add_fk(g_inventor_disambiguated.tsv.head,
              patent_id,
              g_patent.tsv.head)
```

## Five: Try to render that schema:

```{r}
#| label: display schema

schema <- dm_draw(pot_schema_all_keys, view_type = 'all')

# kind of hacky 
schema_html <- DiagrammeRsvg::export_svg(schema)
htmltools::HTML(schema_html )
```

# Producing a summary table

 Sixth: given that logic build a summary table by county/year

 We will probably do it in at least to n steps:

 - Getting counties, right now we do not know if we are missing some counties and we are still unsure what vintage of census patentsview is using.

 - linking patent to assignee then link to location

 - linking patent to inventor then link to location 

```{r}
#| eval: false
# some sanity check: 
dim(g_patent)
dim(g_location_dis)

length(unique(g_assignee_dis$disambig_assignee_organization))

slim_assignee <- g_assignee_dis[!is.na(g_assignee_dis$location_id), c("patent_id", "assignee_id", "location_id", "disambig_assignee_organization")]
dim(slim_assignee)

dim(slim_assignee)

patent_w_assignee <- merge(g_patent, slim_assignee, 
                           by.x = "patent_id", by.y = "patent_id", 
                           all.x  = TRUE, all.y = TRUE) 
dim(patent_w_assignee)    

head(g_location_dis)

# it seems we have some NA in locvation, prob 
# case outside of US 

dim(g_location_dis)

get_me_us_location <- function(dat) {
  slim_location <- dat[dat$disambig_country == "US", c("location_id", "state_fips", "county_fips")]
  slim_location <- slim_location[!is.na(slim_location$county_fips),]
  slim_location$geoid_co <- paste0(sprintf("%02d", slim_location$state_fips),
                                   sprintf("%03d", slim_location$county_fips)) 
  return(slim_location)
}

 
# still has NA (11), sometimes we know the states but sometimes not even



test <- unique(slim_location$state_fips)

sprintf("%02d", test)
sprintf("%03d")


# check the disambig_county US seems googledrive
```